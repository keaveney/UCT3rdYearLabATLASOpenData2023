{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Measuring the Z boson mass\n",
    " This notebook contains basic anlaysis as described in the lab manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0. Importing useful packages\n",
    "\n",
    " Most python scripts start with 'import' statements like this that make useful packages available. Examples are 'uproot' which reads in our data files and allows them to be read, 'numpy' is a powerful and popular package for fast manipultation of arrays and 'scipy.stats' is useful for statitical analysis,we'll use it to generate and fit functions to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lorentz\n",
    "import uproot3 as uproot\n",
    "import uproot_methods.classes.TLorentzVector as LVepm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import cauchy\n",
    "from scipy.stats import norm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Access the data file and convert into a 'pandas dataframe'.\n",
    "\n",
    " - Here the file is converted into the very poweful 'pandas dataframe' format which is used a lot in modern\n",
    "     statistical analysis. You can think of the dataframe as a super-charged spreadsheet. Each row represents \n",
    "     a proton-proton collision event, and every column represents a piece of information ('observable') we have\n",
    "     recorded from that event.\n",
    "\n",
    " - We are going to work with 7 observables: the components of the muon's four-vector, the charge of the muon and two     extra variables related to how isolated the muon is from other particles. As we will have two muons produced in       each event, that makes 14 observables per event.\n",
    " \n",
    " \n",
    "The first thing we want to do is read our data file. Remebmber it contains ~500k proton collision events from the ATLAS Open Data pre-selected so that each event contains two muon candidates with transverse mometum greater than 20000 MeV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_Skim_mumu.root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e541b28a67d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meventsData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muproot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_Skim_mumu.root\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mini\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#this command tells uproot where to find the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meventsData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"muon_E\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"muon_pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"muon_phi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"muon_eta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"muon_charge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"muon_etcone20\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"muon_ptcone30\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File has been successfully opened!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py38_env/lib/python3.8/site-packages/uproot3/rootio.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, localsource, xrootdsource, httpsource, **options)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mopenfcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocalsource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mROOTDirectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenfcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_bytesid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"root\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py38_env/lib/python3.8/site-packages/uproot3/rootio.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mopenfcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMemmapSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mopenfcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocalsource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py38_env/lib/python3.8/site-packages/uproot3/source/memmap.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py38_env/lib/python3.8/site-packages/numpy/core/memmap.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mf_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mf_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mf_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_Skim_mumu.root'"
     ]
    }
   ],
   "source": [
    "eventsData = uproot.open(\"data_Skim_mumu.root\")[\"mini\"] #this command tells uproot where to find the file\n",
    "df = eventsData.pandas.df([\"muon_E\", \"muon_pt\", \"muon_phi\", \"muon_eta\", \"muon_charge\", \"muon_etcone20\", \"muon_ptcone30\"])\n",
    "print(\"File has been successfully opened!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE - NONE OF THE REST OF THIS NOTEBOOK WILL RUN UNTIL YOU DOWNLOAD THE INPUT DATA AND SIMULATION FILES FROM HERE. https://drive.google.com/drive/folders/1iqUqc277CDBXsfUks0Z3i9EzhH0C5_Fp?usp=sharing\n",
    "\n",
    " - You will need to be logged in to google with your UCT account to gain access\n",
    " - PLACE ALL FILES IN THE SAME DIRECTORY AS THIS NOTEBOOK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said dataframes are like spreadsheets and we can see that structure directly by simply typing the name of the variable representing the dataframe (\"df\") into the prompt.\n",
    "We'll see the familiar row/column structure with our twelve observables and lots of events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see each event has values of the variables for \"muon[0]\" and \"muon[1]\". These refer to the muon\n",
    "with the larger and smaller pt values respectively.\n",
    "\n",
    "The next thing we want to do is apply some more critertia to our data so that it is dominated as much as possible by the Z->mumu process while still retaining as many events as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Apply event selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut0: we require the charges of the muons to be opposite (by requiring the sum to be 0)\n",
    "cuts0 = df[ ((df[\"muon_charge[0]\"]) + (df[\"muon_charge[1]\"])  == 0)   ]\n",
    "\n",
    "#cut1: we require the transervse momentum of each muon to be above 20000 MeV'\n",
    "cuts1 = cuts0[(cuts0[\"muon_pt[0]\"] > 20000) & (cuts0[\"muon_pt[1]\"] > 20000 )]\n",
    "\n",
    "#cut2: we require the absolute pseudorapidity of each muon to be below 2.4'\n",
    "cuts2 = cuts1[( np.abs(cuts1[\"muon_eta[0]\"]) < 2.4) & ( np.abs(cuts1[\"muon_eta[1]\"]) < 2.4)]\n",
    "\n",
    "# cuts 2&3: we require the muons to be isolated from other particles in the event these criteria will be\n",
    "# explored later in step 7, for now leave them commented out.\n",
    "\n",
    "cuts3 = cuts2[( (cuts2[\"muon_etcone20[0]\"])/(cuts2[\"muon_pt[0]\"]) < 0.1) & ( (cuts2[\"muon_etcone20[1]\"])/(cuts2[\"muon_pt[1]\"]) < 0.1) ]\n",
    "cuts4 = cuts3[( (cuts3[\"muon_ptcone30[0]\"])/(cuts3[\"muon_pt[0]\"]) < 0.1) & ( (cuts3[\"muon_ptcone30[1]\"])/(cuts3[\"muon_pt[1]\"]) < 0.1) ]\n",
    "\n",
    "#we copy out finally selected dataframe to a new variable 'finalData' for convenience\n",
    "finalData =  cuts4 # you will have to change this line when you want to inlcude the muon \n",
    "#isolation criteria, i.e., \"finalData =  cuts4\" \n",
    "\n",
    "#let's check how many events we have selected after all our criteria have been applied\n",
    "print(\"Number of selected events = \" + str(len(finalData.index)))\n",
    "\n",
    "# and have a look at our finally selected dataframe\n",
    "finalData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 3. 'Reconstruct' the Z boson by adding the four-vectors of each muon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we make arrays of four-vectors for the muons. we make separate arrays for muon[0] (leading) and muon[1] (sub-leading)\n",
    "lvArray0 = LVepm.TLorentzVectorArray.from_ptetaphi(finalData[\"muon_pt[0]\"], finalData[\"muon_eta[0]\"], finalData[\"muon_phi[0]\"],finalData[\"muon_E[0]\"])\n",
    "lvArray1 = LVepm.TLorentzVectorArray.from_ptetaphi(finalData[\"muon_pt[1]\"], finalData[\"muon_eta[1]\"], finalData[\"muon_phi[1]\"],finalData[\"muon_E[1]\"])\n",
    "\n",
    "# the TLorentzVectorArray class conveniently allows us to simply add the array\n",
    "# to get an array of four-vectors representing the dimuon system in each event\n",
    "lvArray = lvArray0 + lvArray1\n",
    "lvArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.  Make histogram plots of the kinematic information of the muon pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make some simple histograms of the kineamtic information associated with the muons.\n",
    "# as an example I look at muon[0] only, but you can look at muon[1] and compare the distributions.\n",
    "\n",
    "# plot the mu[0] pt histogram.\n",
    "plt.figure()\n",
    "plt.xlabel(\"mu pt\")\n",
    "plt.ylabel(\"events per bin\")\n",
    "plt.hist(lvArray0.pt, bins=35, range=[0,120000], alpha=0.6, color='g')\n",
    "\n",
    "# plot the mu[0] pseudorapdity histogram.\n",
    "plt.figure()\n",
    "plt.xlabel(\"mu eta\")\n",
    "plt.ylabel(\"events per bin\")\n",
    "plt.hist(lvArray0.eta, bins=25, range=[-3.0,3.0], alpha=0.6, color='g')\n",
    "\n",
    "# plot the mu[0] phi histogram.\n",
    "plt.figure()\n",
    "plt.xlabel(\"mu phi\")\n",
    "plt.ylabel(\"events per bin\")\n",
    "plt.hist(lvArray0.phi, bins=15, range=[-3.14,3.14], alpha=0.6, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Make histogram plots of the kinematic information of the muons.\n",
    "- now that we have reconstructed four-vectors of the dimuon system and know how to make histogram plots,let's make a histogram plot of the dimuon mass distribution and see if the Cauchy distribution can be seen and if the peak is somehwhere around 90 GeV\n",
    "\n",
    "- we will compare this histogram to a cauchy pdf. However, pdfs are always normalised to an area of 1 while our data represents hundreds of thousands of events, so the data histogram will surely have an integral much larger than 1. Therefore we must first calculate this integral use it to scale our pdf to allow a fair visual comparison between the data histogram and the predicted Cauchy distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we set up a few parmaters for our histogram, min and max of the x-axis (mass)\n",
    "minMass = 71000\n",
    "maxMass = 110000\n",
    "\n",
    "#the number of bins\n",
    "nBins = 250 \n",
    "\n",
    "# creating the histogram as two arrays (bin edges & counts in the bins) numpy\n",
    "countsData, edges = np.histogram(lvArray.mass, bins=nBins, range=(minMass, maxMass))\n",
    "\n",
    "# get the width of each bin\n",
    "bin_width = edges[1] - edges[0]\n",
    "# sum over number in each bin and mult. by bin width, which can be factored out.\n",
    "# This gives us the integral\n",
    "integral = bin_width * sum(countsData[0:nBins])\n",
    "\n",
    "# we can make an array of the centre of each bin directly from the edges array this will be useful in plotting our pdf\n",
    "centres = (edges[1:] + edges[:-1]) / 2\n",
    "\n",
    "#fit a Cauchy distributions to the dimuon mass data\n",
    "#mu, std = cauchy.fit(lvArray.mass)\n",
    "#print(\"mu = \" + str(mu))\n",
    "#print(\"std = \" + str(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are almost ready to compare our data histogram with the predicted Cauchy distribution but we know the Cauchy pdf has two free parameters: the mean (mu) and standard deviation (sigma). what should we choose for these values for a first comparison? Why not the world-average values published by the Particle Data group below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mZPDG  = 91187.6\n",
    "sigmaZPDG = 2495.2\n",
    "\n",
    "#let's make the comparison plot\n",
    "plt.figure()\n",
    "p = (cauchy.pdf(centres, mZPDG, sigmaZPDG) * integral)\n",
    "plt.plot(centres, p, 'k',label=\"Cauchy pdf (PDG values)\", linewidth=2)\n",
    "plt.hist(lvArray.mass, bins=nBins, range=[minMass, maxMass],label=\"ATLAS Data\", alpha=0.6, color='g')\n",
    "plt.legend()\n",
    "plt.xlabel(\"mumu mass [MeV]\")\n",
    "plt.ylabel(\"# events\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a clear peak and a shape that looks very much like a cauchy distrubtion. So we can be confident that our data conatains the decays of a heavy particle to pairs of muons. However the data does not perfectly agree with the predicted shape of a cauchy distributions with the PDG values for the mass and width of the Z boson.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fitting the Cauchy model to the data\n",
    "In the next part of this notebook we will try to assess if we can find values of the Z mass which result in a better 'agreement' between the data and cauchy model. If we trust the cauchy model completely, then this proceedure is a measurement of the z mass. We'll define the agreement between our data and model with the chi-squared function. the function take in arrays of events counts ('obs') and predictions ('preds') corresponding to the bins of the dimuon mass distribution. the function returns the squared difference between the event count and prediction divded by the variance on the prediction, summed over all bins, otherwise known as the chi-squared function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chi-squared function\n",
    "def calcChiSq(obs, preds):\n",
    "    chiSq = 0.0\n",
    "    ndf = len(obs)\n",
    "    for bin in range(0, len(obs)):\n",
    "        diff = preds[bin] - obs[bin]\n",
    "        #print(\"diff = \" + str(diff))\n",
    "        #var = ( np.abs(obs[bin])) # neymans's chi2 (approximate the of the data's variance as poisson)\n",
    "        var = ( np.abs(preds[bin])) # pearson's chi2 (approximate the of the prediction's variance as poisson)\n",
    "        if (var != 0):\n",
    "            chiSq += (diff**2)/(var)\n",
    "            #print(\"obs, pred, diff, var  chi contrib = \"+  str( obs[bin]) +\" \" +  str(preds[bin]) +\" \"+  str(diff) +\" \"+  str(var) +\" \" + str((diff**2)/(var)))\n",
    "    return chiSq, ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the fit we will:\n",
    "- We'll now loop over a range of $m_{Z}$ values, \n",
    "- Generate a cauchy distribution for each value evaluate the $\\chi^{2}$ between that distribution and the data histogram.\n",
    "- find the $m_{Z}$ value that gives the smallest the value of the $\\chi^{2}$\n",
    "- plot the $\\Delta \\chi^{2}$ vs the $m_{Z}$, and use the \\emph{critical values} of this curve to evulated the uncertainties on $m_{Z}$\n",
    "- compare the data histogram to the fitted} Cauchy distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some arrays to hold the z mass and chi-squared values\n",
    "mZAr = np.array([])\n",
    "chi2Ar = np.array([])\n",
    "\n",
    "#deciding the range of z mass values and how fine the steps in this range will be\n",
    "minMz = 90730\n",
    "maxMz = 90765\n",
    "step = 1\n",
    "\n",
    "#vairables to use in the loop to find value of z mass that gives smallest chi2\n",
    "bestFitMz1D = 0.0\n",
    "minChi2 = 10000000\n",
    "\n",
    "for mZ in range(minMz, maxMz, step): #starting loop\n",
    "    countsPDF = (cauchy.pdf(centres, mZ, sigmaZPDG) * integral) #building pdf for these values\n",
    "    #countsPDF = (norm.pdf(centres, mZ, sigmaZPDG) * integral) #building pdf for these values\n",
    "    chi2, ndf = calcChiSq(countsData, countsPDF) # call chi2 function to get chi2 for this particular pdf and data\n",
    "    mZAr = np.append(mZAr, mZ) #adding values to arrays\n",
    "    chi2Ar = np.append(chi2Ar, chi2)\n",
    "    if(chi2 < minChi2): #keeping track of what is the smallest chi2 value we have found\n",
    "        minChi2 = chi2\n",
    "        bestFitMz1D = mZ\n",
    "                \n",
    "# as we will only be interested in the change in chi2 w.r.t. mZ,  we can rescale all the chi2\n",
    "#values such that the minimum chi2 value is 0. \n",
    "chi2Ar = chi2Ar - minChi2\n",
    "\n",
    "#we expect the chi2 vs. mZ curve to be quadratic, so let's fit that fucntion to it.\n",
    "z = np.polyfit(mZAr, chi2Ar, 2) #\"2\" for a second-order polynomial\n",
    "p = np.poly1d(z)\n",
    "\n",
    "#lets plot our chi2 values vs mz along with the fit\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mZAr, chi2Ar, 'r+', lw=5, alpha=0.6, label=\"chi2 evaluations\")\n",
    "ax.plot(mZAr, p(mZAr), 'b-', lw=5, alpha=0.6, label=\"quadratic fit\")\n",
    "\n",
    "# we can display the estimated uncertianty on mZ via critical values () of the delta chi-squared curve\n",
    "y0 = 1.0\n",
    "crit = (p - y0).roots # roots of the polynominal -1, i.e., the mz values where p = 1 \n",
    "\n",
    "#shading in the uncertainty band \n",
    "px=np.arange(crit[1],crit[0],0.001)\n",
    "ax.fill_between(px,p(px),alpha=0.5, color='g', label=\"uncertainty\")\n",
    "ax.legend()\n",
    "\n",
    "#setting reasonable axis ranges and titles\n",
    "ax.set_xlim(minMz, maxMz)\n",
    "ax.set_ylim(0.0, 5.0)\n",
    "plt.xlabel(\"mZ [MeV]\")\n",
    "plt.ylabel(\"delta chi-squared\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#print result and uncertainty\n",
    "my1DResult = bestFitMz1D\n",
    "my1DUncertainty = bestFitMz1D-crit[1]\n",
    "print(\"best-fit = \" + str(my1DResult) + \" +/- \" + str(my1DUncertainty))\n",
    "\n",
    "# we now compare our data histogram with the cauchy distribution with the mean and standard deviation\n",
    "# that minimise the chi-squared. This pdf should agree much better than the pdf with the PDG values.\n",
    "\n",
    "plt.figure()\n",
    "p = (cauchy.pdf(centres, my1DResult, sigmaZPDG))\n",
    "\n",
    "pInt = bin_width * sum(p[0:nBins])\n",
    "scale = integral/pInt\n",
    "pScaled = (cauchy.pdf(centres, my1DResult, sigmaZPDG) *scale)\n",
    "\n",
    "plt.plot(centres, pScaled, 'k', linewidth=2)\n",
    "\n",
    "x = np.linspace(87000, 94000, 1000)\n",
    "pScaledNorm = (norm.pdf(x, my1DResult, (sigmaZPDG*0.8) )*(0.6*scale))\n",
    "\n",
    "#plt.plot(x, pScaledNorm, 'r', linewidth=2)\n",
    "\n",
    "plt.hist(lvArray.mass, bins=nBins, range=[minMass, maxMass], alpha=0.6, color='g')\n",
    "plt.xlabel(\"mumu mass [MeV]\")\n",
    "plt.ylabel(\"# events\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a double gaussian with curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume mass, sigma\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def gaussian(x, mean, amplitude, standard_deviation):\n",
    "    return amplitude * np.exp( - ((x - mean) / standard_deviation) ** 2)\n",
    "\n",
    "def doubleGaussian(x, mean1, amplitude1, standard_deviation1, mean2, amplitude2, standard_deviation2):\n",
    "    gaus1 = amplitude1 * np.exp( - ((x - mean1) / standard_deviation1) ** 2)\n",
    "    gaus2 = amplitude2 * np.exp( - ((x - mean2) / standard_deviation2) ** 2)\n",
    "    return gaus1 + gaus2\n",
    "\n",
    "#bestFitParams, covarianceMatrix = curve_fit(gaussian, centres, countsData, p0=[91000., 2000., 10000.])\n",
    "bestFitParams, covarianceMatrix = curve_fit(doubleGaussian, centres, countsData, p0=[91000., 2000., 10000., 91000, 6000, 2000])\n",
    "\n",
    "x_interval_for_fit = np.linspace(edges[0], edges[-1], 10000)\n",
    "#plt.plot(x_interval_for_fit, gaussian(x_interval_for_fit, *bestFitParams), label='gaussian fit')\n",
    "plt.plot(x_interval_for_fit, doubleGaussian(x_interval_for_fit, *bestFitParams),'r-', linewidth=3, label='double gaussian fit')\n",
    "plt.errorbar(centres, countsData, yerr=np.sqrt(countsData), label=\"Data\", fmt='o', mfc='k', mec='k', mew=0.1, ecolor='k')\n",
    "plt.legend()\n",
    "plt.xlabel(\"$ m_{\\mu\\mu}$\")\n",
    "plt.ylabel(\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have a 'measurement' of mZ, let's make a plot comparing our measured value with that from\n",
    "the PDG to assess how good our measurement is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mZPDGAr = np.array([mZPDG]) \n",
    "uncZPDGAr = np.array([2.1]) \n",
    "yPDGAr = np.array([1.0]) \n",
    "\n",
    "mZ1D = np.array([my1DResult]) \n",
    "sigZ1D = np.array([my1DUncertainty]) \n",
    "y1D = np.array([2.0]) \n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(minMz, maxMz+700)#need to extend x axis range to include PDG value\n",
    "ax.set_ylim(0.0, 4.0)\n",
    "\n",
    "plt.errorbar(mZPDGAr, yPDGAr, xerr=uncZPDGAr, label=\"Particle Data Group\", elinewidth=5,  fmt=\"o\", mfc=\"blue\", ms=8)\n",
    "plt.errorbar(mZ1D, y1D, xerr=sigZ1D, fmt=\"o\", label=\"ATLAS Open Data\", elinewidth=5, ms=8, mfc='blue')\n",
    "\n",
    "plt.xlabel(\"mZ [MeV]\")\n",
    "plt.yticks(y1D, \" \")\n",
    "\n",
    "\n",
    "ax.axes.yaxis.set_visible(False)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats completes steps 1-5. You have seen how the chi-squared minisation technique allows us to extract measurements\n",
    "of a physical parameter such as $m_{Z}$ by comparing the predictions of statistical model to data.\n",
    "\n",
    "# Now it's your turn!\n",
    "\n",
    "Refer back to the lab manual to see how you might improve this measurement and have fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
